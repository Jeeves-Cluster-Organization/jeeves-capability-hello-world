# ==============================================================================
# Jeeves FF - Environment Configuration
# ==============================================================================
#
# Usage:
#   docker compose up -d
#
# Per Engineering Plan v4.2: Single source of truth for configuration.
# These values are also used by docker-compose.yml via ${VAR:-default} syntax.
#
# ==============================================================================

# ------------------------------------------------------------------------------
# DATABASE (PostgreSQL + pgvector)
# ------------------------------------------------------------------------------
DATABASE_BACKEND=postgres
VECTOR_BACKEND=pgvector

# PostgreSQL connection (matches docker-compose.yml defaults)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DATABASE=assistant
POSTGRES_USER=assistant
POSTGRES_PASSWORD=dev_password_change_in_production

# ------------------------------------------------------------------------------
# LLM PROVIDER (llama-server)
# ------------------------------------------------------------------------------
LLM_PROVIDER=llamaserver
LLAMASERVER_HOST=http://localhost:8080
DEFAULT_MODEL=qwen2.5-7b-instruct

# llama-server configuration (used by docker-compose)
# Upgraded to 7B model for better quality (4.7GB fits in 8GB GPU)
# 7B model has significantly better instruction following and less repetition
LLAMA_MODEL=Qwen2.5-7B-Instruct-Q4_K_M.gguf
# CRITICAL: Context window must be >= 16384 for Qwen2.5
# Agents use num_ctx=16384 for consistent context handling.
#
# IMPORTANT: With --parallel N, context is divided across N slots:
#   n_ctx_per_slot = LLAMA_CTX_SIZE / LLAMA_PARALLEL
# So with 16384 / 4 = 4096 per slot, prompts may get truncated!
# Setting LLAMA_PARALLEL=1 ensures full context per request.
LLAMA_CTX_SIZE=16384
LLAMA_GPU_LAYERS=33
LLAMA_PARALLEL=1

# LLM timeout in seconds (default: 300, max: 600)
# With max_tokens=8000, llama-server may need 3-5 minutes for complex prompts
LLM_TIMEOUT=300

# Per-agent model overrides (optional - uses DEFAULT_MODEL if not set)
# Note: llama-server loads a single model, so these are informational only
# PLANNER_MODEL=qwen2.5-7b-instruct-q4_K_M
# VALIDATOR_MODEL=qwen2.5-7b-instruct-q4_K_M
# CRITIC_MODEL=qwen2.5-7b-instruct-q4_K_M
# META_VALIDATOR_MODEL=qwen2.5-7b-instruct-q4_K_M

# Temperature settings (optional - uses config/constants.py defaults if not set)
# PLANNER_TEMPERATURE=0.3
# VALIDATOR_TEMPERATURE=0.3
# CRITIC_TEMPERATURE=0.2
# META_VALIDATOR_TEMPERATURE=0.7

# Cloud providers (only if LLM_PROVIDER != llamaserver)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# AZURE_ENDPOINT=https://...
# AZURE_API_KEY=...

# ------------------------------------------------------------------------------
# API SERVER
# ------------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=DEBUG

# ------------------------------------------------------------------------------
# FEATURES
# ------------------------------------------------------------------------------
MEMORY_ENABLED=true
META_VALIDATION_ENABLED=true

# ------------------------------------------------------------------------------
# HELLO WORLD - WORKSPACE CONFIGURATION (OPTIONAL)
# ------------------------------------------------------------------------------
# HOST_REPO_PATH: Optional workspace mount for the container.
#   - Not required for hello-world capability
#   - Can be used if you want to mount local files into the container
#
# Path on HOST machine (optional)
HOST_REPO_PATH=E:/Cluster/jeeves-capability-code-analysis

# Path inside container (usually leave as default)
REPO_PATH=/workspace

# Auto-index repository on startup (for semantic search)
# Set to false if you want to manually trigger indexing
AUTO_INDEX_ON_STARTUP=true

# ------------------------------------------------------------------------------

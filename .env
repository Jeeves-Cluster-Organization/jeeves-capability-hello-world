# ==============================================================================
# Jeeves FF - Environment Configuration
# ==============================================================================
#
# Usage:
#   docker compose up -d
#
# Per Engineering Plan v4.2: Single source of truth for configuration.
# These values are also used by docker-compose.yml via ${VAR:-default} syntax.
#
# ==============================================================================

# ------------------------------------------------------------------------------
# DATABASE (PostgreSQL + pgvector)
# ------------------------------------------------------------------------------
DATABASE_BACKEND=postgres
VECTOR_BACKEND=pgvector

# PostgreSQL connection (matches docker-compose.yml defaults)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DATABASE=assistant
POSTGRES_USER=assistant
POSTGRES_PASSWORD=dev_password_change_in_production

# ------------------------------------------------------------------------------
# LLM PROVIDER (llama-server)
# ------------------------------------------------------------------------------
LLM_PROVIDER=llamaserver
LLAMASERVER_HOST=http://localhost:8080
DEFAULT_MODEL=qwen2.5-7b-instruct

# llama-server configuration (used by docker-compose)
# Upgraded to 7B model for better quality (4.7GB fits in 8GB GPU)
# 7B model has significantly better instruction following and less repetition
LLAMA_MODEL=Qwen2.5-7B-Instruct-Q4_K_M.gguf
# CRITICAL: Context window must be >= 16384 for Qwen2.5
# Agents use num_ctx=16384 (see agents/code_analysis/*.py).
#
# IMPORTANT: With --parallel N, context is divided across N slots:
#   n_ctx_per_slot = LLAMA_CTX_SIZE / LLAMA_PARALLEL
# So with 16384 / 4 = 4096 per slot, prompts may get truncated!
# Setting LLAMA_PARALLEL=1 ensures full context per request.
LLAMA_CTX_SIZE=16384
LLAMA_GPU_LAYERS=33
LLAMA_PARALLEL=1

# LLM timeout in seconds (default: 300, max: 600)
# With max_tokens=8000, llama-server may need 3-5 minutes for complex prompts
LLM_TIMEOUT=300

# Per-agent model overrides (optional - uses DEFAULT_MODEL if not set)
# Note: llama-server loads a single model, so these are informational only
# PLANNER_MODEL=qwen2.5-7b-instruct-q4_K_M
# VALIDATOR_MODEL=qwen2.5-7b-instruct-q4_K_M
# CRITIC_MODEL=qwen2.5-7b-instruct-q4_K_M
# META_VALIDATOR_MODEL=qwen2.5-7b-instruct-q4_K_M

# Temperature settings (optional - uses config/constants.py defaults if not set)
# PLANNER_TEMPERATURE=0.3
# VALIDATOR_TEMPERATURE=0.3
# CRITIC_TEMPERATURE=0.2
# META_VALIDATOR_TEMPERATURE=0.7

# Cloud providers (only if LLM_PROVIDER != llamaserver)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# AZURE_ENDPOINT=https://...
# AZURE_API_KEY=...

# ------------------------------------------------------------------------------
# API SERVER
# ------------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=DEBUG

# ------------------------------------------------------------------------------
# FEATURES
# ------------------------------------------------------------------------------
MEMORY_ENABLED=true
META_VALIDATION_ENABLED=true

# ------------------------------------------------------------------------------
# CODE ANALYSIS - REPOSITORY CONFIGURATION
# ------------------------------------------------------------------------------
# HOST_REPO_PATH: Path on the HOST machine to the repository you want to analyze.
#   - This path is mounted into the container at /workspace (read-only)
#   - Set this to the absolute path of the codebase you want to analyze
#
# REPO_PATH: Path INSIDE the container where the repository is mounted.
#   - Default: /workspace (matches the volume mount in docker-compose.yml)
#   - Usually you don't need to change this unless you modify the volume mount
#
# Examples:
#   Linux/Mac:   HOST_REPO_PATH=/home/user/projects/my-repo
#   Windows:     HOST_REPO_PATH=C:/Users/user/projects/my-repo
#   WSL2:        HOST_REPO_PATH=/mnt/c/Users/user/projects/my-repo
#
# Usage:
#   1. Set HOST_REPO_PATH to your target repository
#   2. Run: docker compose -f docker/docker-compose.yml up -d
#   3. The code analysis tools will search within /workspace (REPO_PATH)

# Path on HOST machine (required - set this to your target repo)
# For Docker Desktop on Windows, use Windows path format (not WSL /mnt/ paths)
HOST_REPO_PATH=E:/Cluster/jeeves-capability-code-analysis

# Path inside container (usually leave as default)
REPO_PATH=/workspace

# Auto-index repository on startup (for semantic search)
# Set to false if you want to manually trigger indexing
AUTO_INDEX_ON_STARTUP=true

# ------------------------------------------------------------------------------

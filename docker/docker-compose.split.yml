# Docker Compose configuration for Jeeves FF - Split Architecture
#
# Separates HTTP gateway from gRPC orchestrator for scalability.
# Gateway: HTTP/SSE endpoints (FastAPI)
# Orchestrator: gRPC service (agents, tools, database access)
#
# Usage (from project root):
#   Start services:    docker compose -f docker/docker-compose.split.yml up -d
#   Stop services:     docker compose -f docker/docker-compose.split.yml down
#   View logs:         docker compose -f docker/docker-compose.split.yml logs -f
#   Rebuild:           docker compose -f docker/docker-compose.split.yml build
#
# For local development (single machine), this runs all services in Docker.
# For multi-node deployment, see k8s/ directory.

services:
  # ==========================================================================
  # Gateway - HTTP API (FastAPI + gRPC client)
  # ==========================================================================
  gateway:
    container_name: jeeves-gateway
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: gateway
    image: jeeves-gateway:latest

    ports:
      - "${API_PORT:-8000}:8000"

    env_file:
      - ../.env

    environment:
      # gRPC orchestrator connection
      - ORCHESTRATOR_HOST=orchestrator
      - ORCHESTRATOR_PORT=50051
      # API configuration
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      - LOG_FORMAT=json
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    volumes:
      - ../mission_system/static:/app/static:ro

    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

    depends_on:
      orchestrator:
        condition: service_healthy

    networks:
      - jeeves-network

    labels:
      com.jeeves.service: "gateway"
      com.jeeves.role: "http-api"

  # ==========================================================================
  # Orchestrator - gRPC Service (agents, tools, database)
  # ==========================================================================
  orchestrator:
    container_name: jeeves-orchestrator
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: orchestrator
    image: jeeves-orchestrator:latest

    # Expose gRPC port (optional - for debugging/external access)
    ports:
      - "${GRPC_PORT:-50051}:50051"

    env_file:
      - ../.env

    environment:
      # gRPC server configuration
      - GRPC_PORT=50051
      # Database configuration
      - DATABASE_BACKEND=postgres
      - VECTOR_BACKEND=pgvector
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-assistant}
      - DB_USER=${DB_USER:-assistant}
      - DB_PASSWORD=${DB_PASSWORD:-dev_password_change_in_production}
      # LLM configuration
      - LLAMASERVER_HOST=http://llama-server:8080
      - LLM_PROVIDER=${LLM_PROVIDER:-llamaserver}
      # Logging
      - LOG_FORMAT=json
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    volumes:
      - ../data:/app/data

    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; ch=grpc.insecure_channel('localhost:50051'); grpc.channel_ready_future(ch).result(timeout=5)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

    depends_on:
      postgres:
        condition: service_healthy
      llama-server:
        condition: service_healthy

    networks:
      - jeeves-network

    labels:
      com.jeeves.service: "orchestrator"
      com.jeeves.role: "grpc-service"

  # ==========================================================================
  # PostgreSQL Database with pgvector Extension
  # ==========================================================================
  postgres:
    container_name: jeeves-postgres
    image: pgvector/pgvector:pg16

    ports:
      - "${DB_PORT:-5432}:5432"

    environment:
      - POSTGRES_DB=${DB_NAME:-assistant}
      - POSTGRES_USER=${DB_USER:-assistant}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-dev_password_change_in_production}
      - POSTGRES_INITDB_ARGS=-E UTF8 --locale=en_US.UTF-8

    volumes:
      - db-data:/var/lib/postgresql/data
      - ../avionics/database/schemas:/docker-entrypoint-initdb.d:ro

    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

    restart: unless-stopped

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-assistant} -d ${DB_NAME:-assistant}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

    networks:
      - jeeves-network

    labels:
      com.jeeves.service: "database"

  # ==========================================================================
  # llama-server LLM Service
  # ==========================================================================
  llama-server:
    container_name: jeeves-llama-server
    image: ghcr.io/ggerganov/llama.cpp:server

    ports:
      - "${LLAMASERVER_PORT:-8080}:8080"

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    volumes:
      - llama-models:/models

    # CRITICAL: --ctx-size must be >= 8192 to prevent prompt truncation.
    # Agents use up to 8192 tokens (see config/constants.py). Lower values cause
    # the model to "forget" system instructions, leading to invalid JSON output.
    #
    # IMPORTANT: With --parallel N, context is divided: n_ctx_per_slot = ctx_size / N
    # Default parallel=1 ensures full 8192 context per request (no truncation).
    # If you need parallelism, increase ctx-size proportionally (e.g., 32768 for 4 slots).
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - "/models/${LLAMA_MODEL:-qwen2.5-3b-instruct-q4_k_m.gguf}"
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-16384}"
      - --n-gpu-layers
      - "${LLAMA_GPU_LAYERS:-35}"
      - --parallel
      - "${LLAMA_PARALLEL:-1}"

    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '1.0'
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]

    restart: unless-stopped

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    networks:
      - jeeves-network

    labels:
      com.jeeves.service: "llm"

  # ==========================================================================
  # Test Runner Service
  # ==========================================================================
  test:
    container_name: jeeves-test
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: test

    environment:
      - CONTAINER=1
      - DATABASE_BACKEND=postgres
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-assistant}
      - DB_USER=${DB_USER:-assistant}
      - DB_PASSWORD=${DB_PASSWORD:-dev_password_change_in_production}
      - LLAMASERVER_HOST=http://llama-server:8080
      - LLM_PROVIDER=llamaserver
      - ORCHESTRATOR_HOST=orchestrator
      - ORCHESTRATOR_PORT=50051
      - API_HOST=http://gateway:8000
      - LOG_LEVEL=DEBUG

    depends_on:
      postgres:
        condition: service_healthy
      llama-server:
        condition: service_healthy
      orchestrator:
        condition: service_healthy
      gateway:
        condition: service_healthy

    networks:
      - jeeves-network

    command: pytest -v --tb=short

    labels:
      com.jeeves.service: "test"

# =============================================================================
# Networks
# =============================================================================
networks:
  jeeves-network:
    driver: bridge
    labels:
      com.jeeves.network: "main"

# =============================================================================
# Volumes
# =============================================================================
volumes:
  llama-models:
    driver: local
    labels:
      com.jeeves.volume: "models"

  db-data:
    driver: local
    labels:
      com.jeeves.volume: "database"

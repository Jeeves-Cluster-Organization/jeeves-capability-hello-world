# Docker Compose configuration for Jeeves Hello World Capability
#
# Cross-platform: Works on Linux, Windows (Docker Desktop + WSL2), and macOS.
# GPU auto-detection: Falls back to CPU if NVIDIA GPU unavailable.
#
# Profiles:
#   prod:    Production services (orchestrator, gateway, postgres, llama-server)
#   test:    All services including test runner
#   dev:     All services (same as no profile)
#   minimal: Core services only (orchestrator, postgres, llama-server) - no gateway
#
# IMPORTANT: Run from PROJECT ROOT with --env-file flag to ensure HOST_REPO_PATH is read:
#
# Usage (from project root):
#   Build services:         docker compose --env-file .env -f docker/docker-compose.yml build
#   Start all services:     docker compose --env-file .env -f docker/docker-compose.yml up -d
#   Start production:       docker compose --env-file .env -f docker/docker-compose.yml --profile prod up -d
#   Build test image:       docker compose --env-file .env -f docker/docker-compose.yml --profile test build test
#   Run tests:              docker compose --env-file .env -f docker/docker-compose.yml --profile test run --rm test pytest tests/unit/ -v
#   Run gateway tests:      docker compose --env-file .env -f docker/docker-compose.yml --profile test run --rm test pytest tests/integration/test_gateway.py -v
#   Stop services:          docker compose --env-file .env -f docker/docker-compose.yml down
#   View logs:              docker compose --env-file .env -f docker/docker-compose.yml logs -f
#
# Why --env-file is required:
#   Docker Compose variable substitution (${VAR}) in volume mounts reads from:
#   1. Shell environment
#   2. .env in the SAME directory as docker-compose.yml (i.e., docker/.env)
#   Since our .env is in the project root, we must explicitly specify --env-file .env
#
# Build options:
#   Build sequentially:     docker compose build --parallel 1  (for low-memory systems)
#
# Deployment script (recommended):
#   python scripts/deployment/deploy.py --profile prod --build
#   python scripts/deployment/deploy.py --profile test
#   python scripts/deployment/deploy.py --test-gateway
#
# Prerequisites for GPU:
#   Linux:   nvidia-container-toolkit + Docker configured with nvidia runtime
#   Windows: NVIDIA driver 535.54+, WSL 2.0.9+, Docker Desktop with WSL2 backend
#   macOS:   GPU not supported (CPU only)
#
# Verify GPU access:
#   docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

services:
  # ==========================================================================
  # Jeeves Orchestrator Service (gRPC)
  # ==========================================================================
  orchestrator:
    container_name: jeeves-orchestrator
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: orchestrator
      args:
        CODE_VERSION: ${CODE_VERSION:-dev}
    image: jeeves-hello-world:latest

    # Port mapping (host:container) - Gradio UI listens on 8000 inside container
    ports:
      - "${API_PORT:-8000}:8000"
      - "${METRICS_PORT:-9090}:9090"

    # Environment variables (override with .env file)
    env_file:
      - ../.env
    environment:
      - NODE_ID=${NODE_ID:-node-01}
      - NODE_NAME=${NODE_NAME:-assistant-primary}
      # LLM Configuration (new neutral vars)
      - JEEVES_LLM_ADAPTER=${JEEVES_LLM_ADAPTER:-openai_http}
      - JEEVES_LLM_BASE_URL=http://llama-server:8080/v1
      - JEEVES_LLM_MODEL=${JEEVES_LLM_MODEL:-qwen2.5-7b-instruct}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-300}
      # Database Configuration
      - DATABASE_BACKEND=postgres
      - VECTOR_BACKEND=pgvector
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-assistant}
      - DB_USER=${DB_USER:-assistant}
      - DB_PASSWORD=${DB_PASSWORD:-dev_password_change_in_production}
      # API Configuration
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - LOG_FORMAT=json
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Hello World Configuration
      - REPO_PATH=${REPO_PATH:-/workspace}

    # Volume mounts (data persistence)
    volumes:
      - ../data:/app/data
      - ../.env:/app/.env:ro
      # Optional workspace mount (not required for hello-world)
      - ${HOST_REPO_PATH:-..}:/workspace:ro

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

    # Restart policy
    restart: unless-stopped

    # Health check - Gradio UI on port 8000
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/').read()"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

    # Dependencies
    depends_on:
      postgres:
        condition: service_healthy
      llama-server:
        condition: service_healthy

    # Network
    networks:
      - jeeves-network

    # Labels
    labels:
      com.jeeves.service: "orchestrator"
      com.jeeves.capability: "hello-world"
      com.jeeves.version: "1.0.0"


  # ==========================================================================
  # PostgreSQL Database with pgvector Extension
  # ==========================================================================
  postgres:
    container_name: jeeves-postgres
    image: pgvector/pgvector:pg16

    # Port mapping
    ports:
      - "${DB_PORT:-5432}:5432"

    # Environment variables
    environment:
      - POSTGRES_DB=${DB_NAME:-assistant}
      - POSTGRES_USER=${DB_USER:-assistant}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-dev_password_change_in_production}
      - POSTGRES_INITDB_ARGS=-E UTF8 --locale=en_US.UTF-8

    # Volume mounts
    volumes:
      - db-data:/var/lib/postgresql/data
      - ../jeeves_capability_hello_world/database/schemas:/docker-entrypoint-initdb.d/001:ro

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-assistant} -d ${DB_NAME:-assistant}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

    # Network
    networks:
      - jeeves-network

    # Labels
    labels:
      com.jeeves.service: "database"
      com.jeeves.capability: "hello-world"
      com.jeeves.version: "1.0.0"


  # ==========================================================================
  # Jeeves LLM Service (llama-server with GPU auto-detection)
  # Uses llama.cpp server with OpenAI-compatible API on port 8080
  # ==========================================================================
  llama-server:
    container_name: jeeves-llm
    image: ghcr.io/ggerganov/llama.cpp:server-cuda

    # Port mapping
    ports:
      - "${LLAMASERVER_PORT:-8080}:8080"

    # Environment
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    # Volume for models (persistent storage)
    volumes:
      - llama-models:/models

    # Command: Start llama-server with model
    # Override LLAMA_MODEL env var to use a different model
    # Default: Qwen 2.5-3B-Instruct (2GB) - download with scripts/setup/setup-docker-windows.ps1
    # Alternative: Qwen2.5-7B-Instruct-Q4_K_M.gguf (4.68GB, better quality)
    # CRITICAL: --ctx-size must be >= 16384 for Qwen2.5.
    # Agents use num_ctx=16384 for consistent context handling.
    #
    # IMPORTANT: With --parallel N, context is divided: n_ctx_per_slot = ctx_size / N
    # Default parallel=1 ensures full 16384 context per request (no truncation).
    # If you need parallelism, increase ctx-size proportionally.
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - "/models/${LLAMA_MODEL:-qwen2.5-3b-instruct-q4_k_m.gguf}"
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-16384}"
      - --n-gpu-layers
      - "${LLAMA_GPU_LAYERS:-35}"
      - --parallel
      - "${LLAMA_PARALLEL:-1}"

    # GPU configuration - uses GPU if available, falls back to CPU
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '1.0'
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]

    # Restart policy
    restart: unless-stopped

    # Health check - llama-server /health endpoint
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    # Network
    networks:
      - jeeves-network

    # Labels
    labels:
      com.jeeves.service: "llm"
      com.jeeves.capability: "hello-world"
      com.jeeves.version: "1.0.0"


  # ==========================================================================
  # Jeeves Test Runner Service
  # Usage:
  #   docker compose --profile test build test
  #   docker compose run --rm test pytest tests/unit/ -v
  #   docker compose run --rm test pytest tests/integration/test_gateway.py -v
  # ==========================================================================
  test:
    container_name: jeeves-test
    # Profile: only build/start when explicitly requested (reduces default build time)
    profiles:
      - test
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: test
      args:
        CODE_VERSION: ${CODE_VERSION:-dev}

    # Environment variables for testing
    # Per Engineering Plan v4.2: All config explicit in container, no silent defaults
    environment:
      # Container detection (required by tests/config/endpoints.py)
      - CONTAINER=1
      # Database
      - DATABASE_BACKEND=postgres
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=${DB_NAME:-assistant}
      - DB_USER=${DB_USER:-assistant}
      - DB_PASSWORD=${DB_PASSWORD:-dev_password_change_in_production}
      # LLM (new neutral vars)
      - JEEVES_LLM_ADAPTER=${JEEVES_LLM_ADAPTER:-openai_http}
      - JEEVES_LLM_BASE_URL=http://llama-server:8080/v1
      - JEEVES_LLM_MODEL=${JEEVES_LLM_MODEL:-qwen2.5-7b-instruct}
      # API (required by tests/config/endpoints.py)
      - API_HOST=http://orchestrator:8000
      # Gateway (for integration tests)
      - GATEWAY_HOST=http://gateway:8000
      - ORCHESTRATOR_HOST=orchestrator
      # Logging
      - LOG_LEVEL=DEBUG

    # Dependencies
    depends_on:
      postgres:
        condition: service_healthy
      llama-server:
        condition: service_healthy

    # Network
    networks:
      - jeeves-network

    # Default test command (can be overridden)
    command: pytest -v --tb=short

    # Labels
    labels:
      com.jeeves.service: "test"
      com.jeeves.capability: "hello-world"
      com.jeeves.version: "1.0.0"


# =============================================================================
# Networks
# =============================================================================
networks:
  jeeves-network:
    driver: bridge
    labels:
      com.jeeves.network: "hello-world"


# =============================================================================
# Volumes
# =============================================================================
volumes:
  # llama-server models - EXTERNAL volume created by setup script
  # This ensures the script and docker-compose use the same volume
  llama-models:
    external: true

  # Database data (named volume for persistence)
  db-data:
    driver: local
    labels:
      com.jeeves.volume: "database"
      com.jeeves.capability: "hello-world"

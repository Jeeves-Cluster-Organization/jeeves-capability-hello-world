# Single-node deployment for Jeeves Code Analysis
# Derived from config/deployment.py PROFILES["single_node"]
#
# Resource limits based on NodeProfile:
#   vram_gb: 6, ram_gb: 20, max_parallel: 4
#   model: qwen2.5-7b-instruct-q4_K_M.gguf (4.4GB)
#
# All 7 agents run on this single node:
#   perception, intent, planner, traverser, synthesizer, critic, integration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jeeves-orchestrator
  labels:
    app.kubernetes.io/name: jeeves-code-analysis
    app.kubernetes.io/component: orchestrator
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jeeves-code-analysis
      app.kubernetes.io/component: orchestrator
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jeeves-code-analysis
        app.kubernetes.io/component: orchestrator
    spec:
      containers:
        - name: orchestrator
          image: jeeves-code-analysis:latest
          ports:
            - name: grpc
              containerPort: 50051
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "2000m"
          env:
            - name: GRPC_PORT
              value: "50051"
            - name: PIPELINE_MODE
              value: "full"
            - name: LOG_LEVEL
              valueFrom:
                configMapKeyRef:
                  name: jeeves-config
                  key: log_level
            - name: LLAMASERVER_HOST
              valueFrom:
                configMapKeyRef:
                  name: jeeves-config
                  key: llamaserver_host
            - name: POSTGRES_HOST
              valueFrom:
                configMapKeyRef:
                  name: jeeves-config
                  key: postgres_host
            - name: POSTGRES_PORT
              valueFrom:
                configMapKeyRef:
                  name: jeeves-config
                  key: postgres_port
            - name: POSTGRES_DATABASE
              valueFrom:
                configMapKeyRef:
                  name: jeeves-config
                  key: postgres_database
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: jeeves-secrets
                  key: postgres_user
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: jeeves-secrets
                  key: postgres_password
          readinessProbe:
            exec:
              command:
                - python
                - -c
                - "import socket; s=socket.socket(); s.settimeout(2); s.connect(('localhost', 50051)); s.close()"
            initialDelaySeconds: 15
            periodSeconds: 10
          livenessProbe:
            exec:
              command:
                - python
                - -c
                - "import socket; s=socket.socket(); s.settimeout(2); s.connect(('localhost', 50051)); s.close()"
            initialDelaySeconds: 30
            periodSeconds: 30
          volumeMounts:
            - name: workspace
              mountPath: /workspace
              readOnly: true
      volumes:
        - name: workspace
          persistentVolumeClaim:
            claimName: workspace-pvc
---
# LLM Server (llama-server) - GPU-enabled
# Derived from config/deployment.py PROFILES["single_node"]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-server
  labels:
    app.kubernetes.io/name: jeeves-code-analysis
    app.kubernetes.io/component: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jeeves-code-analysis
      app.kubernetes.io/component: llm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jeeves-code-analysis
        app.kubernetes.io/component: llm
    spec:
      containers:
        - name: llama-server
          image: ghcr.io/ggerganov/llama.cpp:server-cuda
          args:
            - --host
            - "0.0.0.0"
            - --port
            - "8080"
            - --model
            - "/models/qwen2.5-7b-instruct-q4_K_M.gguf"
            - --ctx-size
            - "16384"
            - --n-gpu-layers
            - "35"
            - --parallel
            - "4"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            requests:
              memory: "4Gi"
              cpu: "1000m"
              nvidia.com/gpu: "1"
            limits:
              memory: "16Gi"
              cpu: "4000m"
              nvidia.com/gpu: "1"
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 30
          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: true
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

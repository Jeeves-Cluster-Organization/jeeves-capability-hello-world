# Node 1: Fast agents (perception, intent, integration)
# Derived from config/deployment.py PROFILES["node1"]
#
# Resources: 6GB VRAM, 20GB RAM
# Model: qwen2.5-7b-instruct-q4_K_M.gguf (4.4GB)
# Agents: perception, intent, integration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jeeves-node1
  labels:
    app.kubernetes.io/name: jeeves-code-analysis
    app.kubernetes.io/component: node1
    jeeves.io/agents: "perception,intent,integration"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jeeves-code-analysis
      app.kubernetes.io/component: node1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jeeves-code-analysis
        app.kubernetes.io/component: node1
    spec:
      containers:
        - name: orchestrator
          image: jeeves-code-analysis:latest
          ports:
            - name: grpc
              containerPort: 50051
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "2000m"
          env:
            - name: NODE_ID
              value: "node1"
            - name: AGENTS
              value: "perception,intent,integration"
            - name: LLAMASERVER_HOST
              value: "http://llama-server-node1:8080"
        - name: llama-server
          image: ghcr.io/ggerganov/llama.cpp:server-cuda
          args:
            - --host
            - "0.0.0.0"
            - --port
            - "8080"
            - --model
            - "/models/qwen2.5-7b-instruct-q4_K_M.gguf"
            - --ctx-size
            - "16384"
            - --n-gpu-layers
            - "28"
            - --parallel
            - "4"
          ports:
            - name: http
              containerPort: 8080
          resources:
            requests:
              memory: "4Gi"
              nvidia.com/gpu: "1"
            limits:
              memory: "8Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: models
              mountPath: /models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
      nodeSelector:
        jeeves.io/node-type: "standard"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: llama-server-node1
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/component: node1

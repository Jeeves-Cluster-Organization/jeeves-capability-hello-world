# Node 3: Reasoning hub (complex planning, synthesis, validation)
# Derived from config/deployment.py PROFILES["node3"]
#
# Resources: 12GB VRAM, 32GB RAM
# Model: qwen2.5-14b-instruct-q4_K_M.gguf (8.5GB) - larger for reasoning
# Agents: planner, synthesizer, critic
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jeeves-node3
  labels:
    app.kubernetes.io/name: jeeves-code-analysis
    app.kubernetes.io/component: node3
    jeeves.io/agents: "planner,synthesizer,critic"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jeeves-code-analysis
      app.kubernetes.io/component: node3
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jeeves-code-analysis
        app.kubernetes.io/component: node3
    spec:
      containers:
        - name: orchestrator
          image: jeeves-code-analysis:latest
          ports:
            - name: grpc
              containerPort: 50051
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
          env:
            - name: NODE_ID
              value: "node3"
            - name: AGENTS
              value: "planner,synthesizer,critic"
            - name: LLAMASERVER_HOST
              value: "http://llama-server-node3:8080"
        - name: llama-server
          image: ghcr.io/ggerganov/llama.cpp:server-cuda
          args:
            - --host
            - "0.0.0.0"
            - --port
            - "8080"
            - --model
            - "/models/qwen2.5-14b-instruct-q4_K_M.gguf"
            - --ctx-size
            - "16384"
            - --n-gpu-layers
            - "35"
            - --parallel
            - "8"
          ports:
            - name: http
              containerPort: 8080
          resources:
            requests:
              memory: "8Gi"
              nvidia.com/gpu: "1"
            limits:
              memory: "16Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: models
              mountPath: /models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models-pvc
      nodeSelector:
        jeeves.io/node-type: "high-memory"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: llama-server-node3
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/component: node3
